{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 04: Language Models\n",
    "\n",
    "## Checkpoint Due Date (Questions 1-5): Wednesday, May 22 11:59 PM\n",
    "\n",
    "## Due Date: Tuesday, May 28 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint Instructions\n",
    "* The checkpoint requires you to turn in questions 1-5.\n",
    "* The checkpoint will be graded for approximate correctness: easier than the final tests; harder than the doctests.\n",
    "* The checkpoint will count toward the extra-credit grade at the end of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Language Models\n",
    "\n",
    "In this project, you will build *[statistical language models](https://en.wikipedia.org/wiki/Language_model)* using public domain books found on [Project Gutenberg](https://www.gutenberg.org/). Language models attempt to capture the likelihood that a given sequence of words occur in a given \"language\" (the precise term is \"corpus\" or \"corpora\"). Here, \"language\" is a broad term that, in addition to the normal usage, may mean the language of a particular author or style. Applications of language models are found in areas such as language generation, authorship attribution, auto-complete / word-suggestion, and machine translation.\n",
    "\n",
    "In this project, you will learn about a simple but effective type of language model (LM) called N-gram models. Given a sentence (that is, a sequence of words $w = w_1\\ldots w_n$), you would like to understand the probability that sentence is used: \n",
    "$$P(w) = P(w_1,\\ldots,w_n)$$\n",
    "However, sentences are built from words, and the likelihood that a word occurs where it does, depends on the words it follows. This points to using *conditional probability* to understand $P(w)$. That is, we can write:\n",
    "\n",
    "$$\n",
    "P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})\n",
    "$$\n",
    "\n",
    "**Example:** Consider the sentence $w = $ `when I eat pizza, I wipe off the grease.`\n",
    "\n",
    "$$\n",
    "P(\\mbox{when}) \\cdot P(\\mbox{I | when}) \\cdot P(\\mbox{ eat | when I})\\cdot P(\\mbox{ pizza | when I eat}) \\cdot \\ldots \\cdot P(\\mbox{ grease | when I eat pizza, I wipe off the})\n",
    "$$\n",
    "\n",
    "That is, the probability that the sentence occurs is the product of the probability that each subsequent word follows the words that came before. For example, the probability $P(\\mbox{ pizza | when I eat})$ is pretty high, as pizza is something that you eat; thus, this term would contribute to this sentence having a higher probability of occurrence.\n",
    "\n",
    "### N-gram language models\n",
    "\n",
    "In the description above, the probability that any word in the sentence (e.g. 'grease') depends on *every* word that preceded it (e.g. 'when'). However, it's often the case the likelihood a word appears in a sentence is influenced more by *nearby* words. \n",
    "* N-gram language models capture this concept that only nearby words matter; they assume the probability a word occurs only depends on the previous $(N−1)$ words. \n",
    "* That is, an N-gram model says: $P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$.\n",
    "\n",
    "**Example: trigram model**\n",
    "\n",
    "Consider the sentence $w = $ `when I eat pizza, I wipe off the grease.`\n",
    "\n",
    "$$\n",
    "P(\\mbox{when}) \\cdot P(\\mbox{I | when}) \\cdot P(\\mbox{ eat | when I})\\cdot P(\\mbox{ pizza | I eat}) \\cdot \\ldots \\cdot P(\\mbox{ the | wipe off}) \\cdot P(\\mbox{ grease | off the})\n",
    "$$\n",
    "In this example, we see the trigram model doesn't consider the beginning of the sentence when considering the likelihood of the end word 'grease'.\n",
    "\n",
    "**Definition:** Given the descriptions and definitions above, we define the **N-grams of a text** to be the list of sliding windows of length N.\n",
    "\n",
    "For Example, the trigrams of the sentence $w$ in the example above are:\n",
    "```\n",
    "[('when', 'I', 'eat'), ('I', 'eat', 'pizza'), ... , ('wipe', 'off', 'the'), ('off', 'the', 'grease')]\n",
    "```\n",
    "\n",
    "### Computing an N-gram language model\n",
    "\n",
    "N-gram language models are a collection of conditional probabilities for sequences of words of length $N$. For example, in a trigram model, for every 3-word sequence $w_1, w_2, w_3$, you need to compute:\n",
    "$$P(w_1,w_2,w_3) = P(w_3 | w_1, w_2) = \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}$$\n",
    "\n",
    "Where $C(w_1, w_2, w_3)$ is the number of occurrences of the trigram-sequence $w_1, w_2, w_3$ in the dataset and $C(w_1, w_2)$ is the number of occurrences of the bigram-sequence  $w_1, w_2$ in the dataset.\n",
    "\n",
    "For a general N-gram model, the conditional probabilities are computed by dividing the counts of N-grams by the (N-1)-grams they follow.\n",
    "\n",
    "\n",
    "### Tokenizing corpora\n",
    "\n",
    "Computing the probabilities of a language model from a book requires breaking up the text of book into sequences of words. This process is called *tokenization*. In reality though, the sequences are not made up entirely of words, but rather more general terms called *tokens*. In this project tokens will include not only whole words, but also punctuation and other terms. Below are a few examples of other types of tokens:\n",
    "\n",
    "* Punctuation like `,.;'`. For example, the period makes sense as a token, as certain words tend to end sentences (i.e. come before a `.` in an N-gram) and begin sentences (i.e. come after a `.` in an N-gram).\n",
    "* We have special 'START' and 'END' tokens that begin and end every word sequence (in our case, paragraphs of words in a given book). These make sense as tokens, as certain words may tend to begin and end paragraphs. \n",
    "\n",
    "It is useful for the tokens used to represent START and END to be single characters that can never be found in the text of the book you use to create our language model. Thus, you will use two ascii hidden \"[control characters](https://en.wikipedia.org/wiki/C0_and_C1_control_codes#C0_(ASCII_and_derivatives))\":\n",
    "* For START, you will use the character `\\x02` (used for 'beginning of text')\n",
    "* For END, you will use the character `\\x03` (used for 'end of text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on checking your work for correctness\n",
    "\n",
    "Building a statistical model involves stringing together difficult-to-follows steps such as data-cleaning and implementing derived mathematical formulas. A fact of life in this line of work is that it can be difficult to assess the correctness of your work. \n",
    "\n",
    "* The Doctests will include checks on the inputs/outputs of your methods.\n",
    "* Doctests will include models on which you can check your work 'by hand'.\n",
    "* Additionally, **you should run your functions on *real* books and assess the reasonableness of your result** as well. Merely passing the doctests is never sufficient to guarantee correctness of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Outline\n",
    "\n",
    "Below is an outline of the the project. If there are terms you don't understand in following descriptions, you should review the introduction for an explanation of the terms and ideas described.\n",
    "\n",
    "1. Write code to download the text of a book from a url, returning the corpus as a string.\n",
    "1. Tokenize the corpus into sequences of words and paragraphs.\n",
    "1. Create two \"baseline\" langunage models; create text with them using sampling.\n",
    "1. Compute an N-gram language model\n",
    "1. Write code to sample from the N-gram language model; create text with them.\n",
    "1. Evaluate performance (computational) of your language model for different N.\n",
    "1. Evaluate performance (statistical) of your language model for different N.\n",
    "1. Create an auto-completion suggestion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import project04 as proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the book(s)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "For this question, you will use `requests` to download and prepare a public domain book from [Project Gutenberg](). Create a function `get_book` that takes in the `url` of a 'Plain Text UTF-8' book and returns a string containing the contents of the book. \n",
    "\n",
    "The function should satisfy the following conditions:\n",
    "* The contents of the book consist of everything between Project Gutenberg's START and END comments.\n",
    "* The contents *will* include title/author/table of contents.\n",
    "* You should also transform any Windows new-lines (`\\r\\n`) with standard new-lines (`\\n`).\n",
    "* You should check the site's `robots.txt` as well and implement a 'pause' in your request in accordance with the website's policy. If the function is called twice in succession, it should not violate the `robots.txt` policy.\n",
    "\n",
    "*Note:* You are encouraged to find whatever books on the website that interest you to test your code and the language models you develop. The text doesn't even need to be an English-language book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/files/57988/57988-0.txt'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_split = [resp.text[:1000], resp.text[1000:-20000], resp.text[-20000:]]\n",
    "text1 = re.sub('[\\\\r]' , '', text_split[0])\n",
    "text1 = re.sub('(.|\\\\n)*\\*\\*\\*( START OF.+)\\*\\*\\*' , '', text1)\n",
    "\n",
    "text2 = re.sub('[\\\\r]' , '', text_split[1])\n",
    "\n",
    "text3 = re.sub('[\\\\r]' , '', text_split[2])\n",
    "text3 = re.sub('(\\*\\*\\* END OF)(.|\\\\n)*', '', text3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the actual book text\n",
    "url = 'http://www.gutenberg.org/files/74/74-0.txt'\n",
    "book_string = proj.get_book(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Corpus\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "Now you need to tokenize the corpus in `book_string` (turn the text into a list of tokens -- i.e. words and punctuation). More specifically, create a function `tokenize` that takes in `book_string` and outputs a list of tokens satisfying the following conditions:\n",
    "* The start of any paragraph should be represented in the list with the single character `\\x02` (standing for START).\n",
    "* The end of any paragraph should be represented in the list with the single character `\\x03` (standing for STOP).\n",
    "* Tokens in the sequence of words are split apart at 'word boundaries' (see the regex lecture).\n",
    "* Tokens should include *no* whitespace.\n",
    "\n",
    "For example, the following sentence (the ellipses denote preceding/continuing text):\n",
    "```\n",
    "...\n",
    "My phone's dead.\n",
    "\n",
    "I didn't get your call.\n",
    "...\n",
    "```\n",
    "should tokenize to:\n",
    "```\n",
    "[ ...\n",
    "\"My\", \"phone\", \"'\", \"s\", \"dead\", \".\", \"\\x03\", \"\\x02\", \"I\", \"didn\", \"'\", \"t\", \"get\", \"your\", \"call\", \".\"\n",
    "... ]\n",
    "```\n",
    "\n",
    "*Remark:* Your tokenization function should run quickly; you should avoid loops when possible. Your `tokenize` function should run on the the complete works of Shakespeare (in `data/shakespeare`) in **under 10 seconds** to get full credit. Be sure to only request the book once (using your `get_book` function) and save it to file for testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\x02',\n",
       " 'My',\n",
       " 'phone',\n",
       " \"'\",\n",
       " 's',\n",
       " 'dead',\n",
       " '.',\n",
       " '\\x02',\n",
       " '\\x03',\n",
       " 'I',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'get',\n",
       " 'your',\n",
       " 'call',\n",
       " '.',\n",
       " '\\x03']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '''\n",
    "My phone's dead. \n",
    "\n",
    "I didn't get your call.\n",
    "'''\n",
    "test1 = '\\x02' + test + '\\x03'\n",
    "test1 = re.sub('\\\\n\\\\n', '\\x02 \\x03', test1)\n",
    "test1 = re.sub('\\\\n', ' ', test1)\n",
    "\n",
    "split_by = r'\\w+|[^\\w\\s]+'\n",
    "r = re.findall(split_by , test1)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = proj.tokenize(book_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shakespeare_fp = os.path.join('data', 'shakespeare.txt')\n",
    "shakespeare = open(shakespeare_fp, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671 ms ± 16.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit proj.tokenize(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Language Model\n",
    "\n",
    "Now that you've learned how to (theoretically) build a language model, you have to implement it in code. Every language model you build will be a class with a few methods in common:\n",
    "\n",
    "* The `__init__` constructor: when you instantiate an LM object, you will need to pass the 'training corpus' on which your model will be trained (i.e. a list of tokens you created above with `tokenize`). The `train` method will then use that data to create a model which is saved in the `mdl` attribute. This code is given to you.\n",
    "* The `train` method takes in a list of tokens (e.g. the output of `tokenize`) and outputs a language model. This language model is usually represented as a `Series` (indexed by tokens; values are probabilities that token occurs), or a `DataFrame`.\n",
    "* The `probability` method takes in a sequence of tokens and returns the probability that this sequence occurs under the language model.\n",
    "* The `sample` method takes in a number `N > 0` and generates a string made up of `N` tokens using the language model. This method generates language.\n",
    "\n",
    "You will create three language model classes in the problems below (Uniform; Unigram; N-gram). For each of these, you will implement each of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models: uniform and unigram LMs\n",
    "\n",
    "### Uniform probability language model\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "A uniform language model is one in which any word is equally likely to appear in any position. The starter code contains a class `UniformLM` which represents a uniform language model. Review the specifics of the class methods above (and in the starter-code) and implement the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tokens = tuple('one one two three one two four'.split())\n",
    "uniform_prob = 1 / pd.Series(test_tokens).nunique()\n",
    "index = pd.Series(test_tokens).unique()\n",
    "mdl = pd.Series(uniform_prob, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = 1\n",
    "for token in ('one', 'two'):\n",
    "    if token not in mdl.index:\n",
    "        prob = 0\n",
    "        break\n",
    "    prob *= mdl[token]\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'three three three one two one four four three two'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = ''\n",
    "M = 10\n",
    "for _ in np.arange(M):\n",
    "    word += np.random.choice(mdl.index, p=mdl.values) + ' '\n",
    "word.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'august stood downstairs crossstreet laps command arms withal druther cutting Bill burned dejected fooling Vision sell Their grotesque teachers dangling Came doubt snake Scratch tripped creak coveted Thomas envying fifteen water miserable visitors lordy poured Planned amid free mortified surer stump gladhearted ablaze beating gifts closing tails fish apartment wher thinking mile talent grimly seeing _any_body townward wretch air Island distinct Bug commanders seize shook Hang Anyway them stooped spied ranges breakfast Order BECKY accidentally log oppressive removal dignified magnified argue extremely shoulder fight market peg fast faintly convenience feeble burden profaned dry planking previous clown Goodmorning chosen fooling growth'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unif = proj.UniformLM(tokens)\n",
    "unif.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram language model\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "A unigram language model is one in which the likelihood a word appears is proportional to its occurrence in the text. That is, it's just the (unconditional) empirical distribution of words in the corpus. The starter code contains a class `UnigramLM` which represents a unigram language model. Review the specifics of the class methods above (and in the starter-code) and implement the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tokens = tuple('one one two three one two four'.split())\n",
    "\n",
    "mdl = pd.Series(test_tokens).value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12244897959183672"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = 1\n",
    "for token in ('one', 'two'):\n",
    "    if token not in mdl.index:\n",
    "        prob = 0\n",
    "        break\n",
    "    prob *= mdl[token]\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'four three one two one two one four two four'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = ''\n",
    "M = 10\n",
    "for _ in np.arange(M):\n",
    "    word += np.random.choice(mdl.index, p=mdl.values) + ' '\n",
    "word.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sleep over -- along once ; as he at hoop expected schoolmates that s he and his until about alleys ' who nature along so of and get who , of bullyrag treacherous himself schoolmaster eminence , vice , “ the ashes teachers slightest Tom on contrived whack A \\x02 pause again invariably , place wish right than !-- it \\x02 “ troubled a -- on her that absurd in . , some \\x03 as one , crybabies They of . ' -- the Sermon people .\\x03 ,” it now treachery spend either - Sighing They the spite me Well\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram = proj.UnigramLM(tokens)\n",
    "unigram.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Baseline models\n",
    "\n",
    "You've now trained two baseline language models capable of generating new text from a given training text. Attempt to answer these questions for yourself before you continue.\n",
    "\n",
    "* Which model do you think is better? Why?\n",
    "* What are the ways in which both of these models are bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Model\n",
    "\n",
    "Now you will build an N-gram language model, which we be a little more involved. The `NGramLM` class contains a few extra methods and attributes beyond those of `UniformLM` and `UnigramLM`:\n",
    "\n",
    "1. Instantiating `NGramLM` requires both a list of tokens and an `N > 0`, specifying the `N` in N-grams. This parameter is stored in an attribute `.N`.\n",
    "1. The `NGramLM` class has a method `create_ngrams` that takes in a list of tokens and returns a list of N-grams (an N-gram is a `tuple` of length `N`). This list of N-grams is then passed to the `train` method to train the N-gram model stored in the `mdl` attribute.\n",
    "1. While the `train` method still creates a language-model (in this case, an N-gram model), this model is most naturally stored as a DataFrame. This DataFrame will have three columns:\n",
    "    - `'ngram'`: containing the n-grams found in the text.\n",
    "    - `'n1gram'`: containing the (n-1)-grams upon which the n-grams in `ngram` are built.\n",
    "    - `'prob'`: containing the probabilities of each n-gram in `ngram`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating n-grams\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "When computing the likelihood that a word occurs in an N-gram model, you compute the probability that word occurs conditional on the (N-1) words that came before it. Therefore, you need to compute a list of n-grams out of your list of tokens (see the introduction for the definition).\n",
    "\n",
    "*Remark*: What if a word doesn't have (N-1) words preceding it? (e.g. it's near the beginning of the text). How do we compute the likelihood of such a word occurs conditional on the (N-1) words that precede it? To handle this corner case, you should repeat the START (`\\x02`) and STOP (`\\x03`) tokens (N-1)-times. Thus, if you want to compute the likelihood of the first word $w=w_1$ in the text, you would compute:\n",
    "$$P(w) = P(w_1) = P(w_1|\\texttt{\\\\x02},\\ldots,\\texttt{\\\\x02})$$\n",
    "\n",
    "Create a method of `NGramLM` called `create_ngrams` that takes in a list of tokens and returns a list of N-grams. The START/STOP tokens in the N-grams should be handled as explained in the remark above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\x02', '\\x02', '\\x02', '\\x02'),\n",
       " ('\\x02', '\\x02', '\\x02', 'one'),\n",
       " ('\\x02', '\\x02', 'one', 'two'),\n",
       " ('\\x02', 'one', 'two', 'three'),\n",
       " ('one', 'two', 'three', 'one'),\n",
       " ('two', 'three', 'one', 'four'),\n",
       " ('three', 'one', 'four', '\\x03'),\n",
       " ('one', 'four', '\\x03', '\\x03'),\n",
       " ('four', '\\x03', '\\x03', '\\x03'),\n",
       " ('\\x03', '\\x03', '\\x03', '\\x03')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "self = proj.NGramLM(4, [])\n",
    "\n",
    "ngrams = []\n",
    "num_preceding = 0\n",
    "start_dist = 1\n",
    "for i in np.arange(len(test_tokens)):\n",
    "\n",
    "    if test_tokens[i] == '\\x02':\n",
    "        gram = (test_tokens[i],) * self.N\n",
    "        ngrams.append(gram)\n",
    "        num_preceding = 0\n",
    "        start_dist = 1\n",
    "        continue    \n",
    "        \n",
    "    if test_tokens[i] == '\\x03':\n",
    "        k = 0\n",
    "        while k < self.N:\n",
    "            gram = ()\n",
    "            for j in np.arange(self.N - 1 - k, 0, -1):\n",
    "                gram += (test_tokens[i - j],)\n",
    "            gram += ('\\x03',) * (k + 1)\n",
    "            ngrams.append(gram)\n",
    "            k += 1\n",
    "            \n",
    "    elif num_preceding < self.N - 1:\n",
    "        gram = (test_tokens[i - start_dist],) * (self.N - 1 - num_preceding)\n",
    "        for j in np.arange(num_preceding, -1, -1):\n",
    "            gram += (test_tokens[i - j],)\n",
    "        ngrams.append(gram)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        gram = ()\n",
    "        for j in np.arange(self.N - 1, -1, -1):\n",
    "            gram += (test_tokens[i - j],)\n",
    "        ngrams.append(gram)\n",
    "        \n",
    "    num_preceding += 1\n",
    "    start_dist += 1\n",
    "    \n",
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the N-gram LM\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "Now, you will compute the probabilities that define N-gram language model itself. Recall that the N-gram LM consists of probabilities\n",
    "$$P(w_1,\\ldots,w_N) = P(w_N | w_1,\\ldots, w_{N-1}) = \\frac{C(w_1, \\ldots, w_N)}{C(w_1,\\ldots, w_{N-1})}$$\n",
    "\n",
    "for every n-gram $(w_1, \\ldots, w_N)$ that occurs in the text.\n",
    "\n",
    "Create a method of `NGramLM` called `train` that takes in a list of N-grams and returns a dataframe of conditional probabilities with the following columns:\n",
    "\n",
    "* `ngram` contains each N-gram that occurs in the text,\n",
    "* `n1gram` contains the (N-1) tokens that precede the last token in the N-gram occupying the same row.\n",
    "* `prob` contains the conditional probabilities associated to the N-gram occupying the same row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>n1gram</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(\u0002, \u0002)</td>\n",
       "      <td>(\u0002,)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(\u0002, one)</td>\n",
       "      <td>(\u0002,)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(one, two)</td>\n",
       "      <td>(one,)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(two, three)</td>\n",
       "      <td>(two,)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(three, one)</td>\n",
       "      <td>(three,)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(one, four)</td>\n",
       "      <td>(one,)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(four, \u0003)</td>\n",
       "      <td>(four,)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(\u0003, \u0003)</td>\n",
       "      <td>(\u0003,)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ngram    n1gram  prob\n",
       "0        (\u0002, \u0002)      (\u0002,)   0.5\n",
       "1      (\u0002, one)      (\u0002,)   0.5\n",
       "2    (one, two)    (one,)   0.5\n",
       "3  (two, three)    (two,)   1.0\n",
       "4  (three, one)  (three,)   1.0\n",
       "5   (one, four)    (one,)   0.5\n",
       "6     (four, \u0003)   (four,)   1.0\n",
       "7        (\u0003, \u0003)      (\u0003,)   1.0"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "self = proj.NGramLM(2, test_tokens)\n",
    "\n",
    "ngram = pd.Series(self.ngrams)\n",
    "n1gram = ngram.apply(lambda x: x[:-1])\n",
    "\n",
    "ngram_unigram = proj.UnigramLM(ngram)\n",
    "n1gram_unigram = proj.UnigramLM(n1gram)\n",
    "prob = ngram.apply(lambda x: ngram_unigram.mdl[x]) / n1gram.apply(lambda y: n1gram_unigram.mdl[y])\n",
    "\n",
    "pd.DataFrame({'ngram': ngram, 'n1gram': n1gram, 'prob': prob})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing probabilities from the N-gram LM\n",
    "\n",
    "**Question 7**\n",
    "\n",
    "Create a method of `NGramLM` called `probability` that takes in a 'sentence' (sequence of tokens) and returns the likelihood of the sentence under the language model (see the introduction of the formulas).\n",
    "\n",
    "*Remark:* What is the probability of a sentence that contains a 'never before seen' word? What about a 'never before seen' (N-1)-gram? Is this reasonable? (Answer: no, it's not reasonable. Fixing this sort of deficiency is a topic in NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "self = proj.NGramLM(2, test_tokens)\n",
    "test_words = 'one two three'.split()\n",
    "\n",
    "prob = 1\n",
    "words_ngram = proj.NGramLM(self.N, test_words).ngrams[1:]\n",
    "for ngram in words_ngram:\n",
    "    if ngram not in self.mdl['ngram'].to_list():\n",
    "        prob = 0\n",
    "        break\n",
    "    prob *= self.mdl.loc[self.mdl['ngram'] == ngram, 'prob'].max()\n",
    "    \n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>n1gram</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(\u0002, \u0002)</td>\n",
       "      <td>(\u0002,)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(\u0002, one)</td>\n",
       "      <td>(\u0002,)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(one, two)</td>\n",
       "      <td>(one,)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(two, three)</td>\n",
       "      <td>(two,)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(three, one)</td>\n",
       "      <td>(three,)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(one, four)</td>\n",
       "      <td>(one,)</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(four, \u0003)</td>\n",
       "      <td>(four,)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(\u0003, \u0003)</td>\n",
       "      <td>(\u0003,)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ngram    n1gram  prob\n",
       "0        (\u0002, \u0002)      (\u0002,)   0.5\n",
       "1      (\u0002, one)      (\u0002,)   0.5\n",
       "2    (one, two)    (one,)   0.5\n",
       "3  (two, three)    (two,)   1.0\n",
       "4  (three, one)  (three,)   1.0\n",
       "5   (one, four)    (one,)   0.5\n",
       "6     (four, \u0003)   (four,)   1.0\n",
       "7        (\u0003, \u0003)      (\u0003,)   1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the N-gram LM\n",
    "\n",
    "**Question 8**\n",
    "\n",
    "As you saw creating the baseline models, sampling from your trained LM provides a way to generate new language. You will code up a `sample` method for `NGramLM` and observe how much better (or worse) the generated language appears when compared to your baseline models.\n",
    "\n",
    "Create a method of `NGramLM` called `sample` that takes in a number `M > 0` and generates a string of M tokens using the language model (not counting the initial START token(s)). It should sample a first token of the form $(\\texttt{\\\\x02},\\ldots,\\texttt{\\\\x02}, w_1)$ from the probabilities in `self.mdl` and continue picking words conditional on the previous choice. Helper functions and recursion will be very helpful.\n",
    "\n",
    "*Remark:* If you find, at any point, your LM has no words to sample, then you should return the STOP token (`\\x03`) and continue until you reach the required length.\n",
    "\n",
    "*(Fun) Remark:* Try generating text using different values of `N`. What are the differences (qualitatively)? Don't be alarmed if your samples generate strange quasi-coherent sentences; it's just randomness!\n",
    "\n",
    "*(Fun) Remark:* Try sampling from an `N`-gram model built on books written in different languages (It doesn't even need to be written using the Latin alphabet!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self = proj.NGramLM(2, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tokens = tuple('\\x02 one two three one four \\x03 \\x02 five one seven dick six one \\x03'.split())\n",
    "\n",
    "def sample(length):\n",
    "\n",
    "    def pick_sample(n1gram, M, num_picked):\n",
    "        \n",
    "        if M > 0 and num_picked > self.mdl.shape[0] - (self.N-1)*2:\n",
    "            return '\\x03 ' * M\n",
    "        \n",
    "        if n1gram[-1] == '\\x03':\n",
    "            if M > 1:\n",
    "                return '\\x02 ' + pick_sample(('\\x02',)*(self.N-1), M-1, num_picked + 1)\n",
    "            else:\n",
    "                return ''\n",
    "        \n",
    "        temp = self.mdl.loc[self.mdl['n1gram'] == n1gram].drop_duplicates()\n",
    "        choices = temp['ngram'].apply(lambda x: x[-1]).tolist()\n",
    "        p = temp['prob'].tolist()\n",
    "        \n",
    "        if(temp.shape[0] == 0):\n",
    "            print(n1gram)\n",
    "        if M == 1:\n",
    "            return np.random.choice(choices, p = p)\n",
    "        \n",
    "        else:\n",
    "            w = np.random.choice(choices, p = p)\n",
    "            n1gram = (n1gram + (w,))[1:]\n",
    "            return w + ' ' + pick_sample(n1gram, M-1, num_picked+1)\n",
    "        \n",
    "    sampled_tokens = '\\x02 '\n",
    "    sampled_tokens += pick_sample(('\\x02',)*(self.N-1), length, 0)\n",
    "    return sampled_tokens.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\x02 “ There was a second attempt to this lady to get around . Say , they worried through the clock he was no loving you get laughed at your way to get me Tom apprehensive . “ Yes , no end . They assembled in it transpired that looked his success which was coming .... Here was a teaspoonful and so few young . The Church -- he had such provision as to one , saw anybody ; it again , and sad business and see now that if determined to ' ll never speak , Huck . \\x03\""
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/files/74/74-0.txt'\n",
    "book_string = proj.get_book(url)\n",
    "tokens = proj.tokenize(book_string)\n",
    "\n",
    "ngram = proj.NGramLM(3, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0002 \u0002 There was a chronic misery . It was a subscriber for all the doorkeys you can ' t stand it , and the widow ' s a box , I don ' know no kings , Tom ? Didn ' t talk .” \u0003 \u0002 The Discovery \u0003 \u0002 \u0002 Jim \u0003 \u0002 “ Most always -- it was in a year since the boys say if they get him ?” \u0003 \u0002 “ Married !” \u0003 \u0002 “ And you ' re hot . Cold again . A measured , solemn tone : \u0003 \u0002 \u0002 \u0002 The\n"
     ]
    }
   ],
   "source": [
    "print(ngram.sample(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the next word in a sentence\n",
    "\n",
    "**Question 9**\n",
    "\n",
    "Next, you will build a predictor that predicts the most likely word to follow a given sentence. The predictions will be the *maximum likelihood estimate* given by your N-gram model. Recall that your N-gram model contains the probabilities that a token follows an (n-1)-gram; your predictor will pick the token with the highest probability of occurring. \n",
    "\n",
    "Create a function `predict_next` that takes in an instantiated `NGramLM` object and a list of tokens, and predicts the *most likely token* to follow the list of tokens in the input (according to `NGramLM`).\n",
    "\n",
    "*Remark:* For which N does this predictor work best? (Determine this by getting a feel for the output, we won't precisely answer this question) .\n",
    "\n",
    "*Remark:* What would this function look like if it were to take in `UniformLM` or `UnigramLM`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl = tuple('\\x02 two one two three two one two three one four \\x03'.split())\n",
    "lm = proj.NGramLM(3, mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tokens = ('one', 'two', 'three')\n",
    "def predict_next(lm, tokens):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 'two'), ('two', 'three')]"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=proj.NGramLM(lm.N - 1, test_tokens)\n",
    "\n",
    "f=lm.mdl.loc[lm.mdl['n1gram'] == t.ngrams[-1]]\n",
    "f.loc[f['prob'].idxmax()].ngram[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation (Extra Credit)\n",
    "\n",
    "**Question 10**\n",
    "\n",
    "This question attempts to answer the question \"which choice of `N` is best when creating an N-gram model?\"\n",
    "\n",
    "As you likely noticed when generating text using your N-gram models, the sentences appear to become more coherent as N increases. The naive conclusion would be to make N very large. However, such a choice has a downside that you will investigate.\n",
    "\n",
    "One concern is that as N gets larger, the distribution of N-grams get more *sparse* -- i.e. the collection of n-grams all have counts close to 1. This makes the language model more likely to merely generate text that was already present in the original text, as opposed to generating *new* language (why?). \n",
    "\n",
    "In this question, you will quantify this observation: given an N-gram model and a sample of length M, what percentage of (N+1)-grams in the sample are present in the text that was used to create the model?\n",
    "\n",
    "Create a function `evaluate_models` which takes in a list of tokens and:\n",
    "1. Generates N-gram models for `N=2,3,4,5,6,7,8` from the given list of tokens,\n",
    "2. Creates samples of length 1000 from each model (remove START/STOP tokens from the samples),\n",
    "3. Calculates the proportion of (N+1)-grams in each sample that was already present in the original list of tokens.\n",
    "\n",
    "The function should return the proportions in a list of length 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
